YOU ARE THE LLM-PROXY 
## High-Level Architecture
We introduce Redis-backed queues, but **each service owns its own queue** and only enqueues into it.

The workflow is:
1) **Jarvis Recipes Server** creates/updates the recipe parsing job record (existing flow) and determines it needs LLM structuring.
2) Instead of calling `llm-proxy` synchronously, Recipes Server calls an **LLM Proxy enqueue endpoint**.
3) **Jarvis LLM Proxy** receives the HTTP request and **enqueues the work into its own Redis queue** (owned/managed by llm-proxy).
4) LLM Proxy processes the job asynchronously.
5) When finished (success or failure), LLM Proxy calls a **Recipes Server callback endpoint**.
6) Recipes Server receives the callback and **updates its own job status store/DB** (and optionally enqueues follow-up work in its own queue).
7) Client continues polling the existing `/recipes/jobs/{job_id}` endpoint; Recipes Server serves status/results from its own job store.

This avoids cross-service direct writes into Redis while keeping the async pipeline.

### Why this pattern
- Clear ownership boundaries (no service writes into another service’s Redis)
- Easier to secure: only allow specific callback endpoints
- Easier to operate: each service can change its internal queue implementation without breaking the other

## Queue Technology Choice
- **Recipes Server**: keep whatever queue mechanism it already uses (RQ, raw Redis, etc.).
- **LLM Proxy**: use **RQ** (recommended) or Redis Streams internally.

The only contract between services is HTTP + JSON.

## Data Contract

### Enqueue Request Payload (Recipes -> LLM Proxy)
```json
{
  "job_id": "uuid",
  "job_type": "recipe_parse_from_ocr_text",
  "created_at": "ISO-8601",
  "priority": "normal|high",
  "trace_id": "uuid-or-string",
  "idempotency_key": "uuid-or-string",
  "job_type_version": "v1",
  "ttl_seconds": 86400,

  "request": {
    "artifacts": {
      "input_uri": "s3://bucket/path/to/input.txt",
      "schema_uri": "s3://bucket/path/to/schema.json",
      "prompt_uri": "s3://bucket/path/to/prompt.json"
    },
    "model": "string",
    "messages": [
      {"role": "system", "content": "..."},
      {"role": "user", "content": "..."}
    ],

    "response_format": {
      "type": "json_object",
      "json_schema": {"...": "..."}
    },

    "sampling": {
      "temperature": 0.2,
      "top_p": 0.95,
      "max_tokens": 2048,
      "seed": 42
    },

    "timeouts": {
      "overall_seconds": 120,
      "per_attempt_seconds": 45
    }
  },

  "callback": {
    "url": "https://recipes.../internal/llm/callback",
    "auth": {
      "type": "bearer",
      "token": "<service-to-service-token>"
    }
  }
}
```

Notes:
- `job_id` is generated by Recipes Server and is the stable correlation id.
- `response_format.json_schema` is required when using `type=json_object` so llm-proxy can validate output.
- `seed` should be supported when available to reduce variance.
- `idempotency_key` is required; recommended to use `job_id`. LLM Proxy must dedupe enqueues by `(job_id, idempotency_key)` for a configurable TTL.
- `ttl_seconds` is required; LLM Proxy should reject or fail jobs that are expired and should not keep retrying beyond TTL.
- `request.artifacts` is optional. If provided, LLM Proxy should fetch inputs from object storage and prefer them over large inline `messages` to keep Redis payloads small.
- `job_type_version` allows evolving job contracts safely across deploys.

### Callback Payload (LLM Proxy -> Recipes)
```json
{
  "job_id": "uuid",
  "job_type": "recipe_parse_from_ocr_text",
  "finished_at": "ISO-8601",
  "status": "succeeded|failed",

  "attempts": 3,
  "timing": {
    "queue_wait_ms": 1200,
    "processing_ms": 18340,
    "total_ms": 19540
  },

  "result": {
    "content": {"...": "valid json object matching schema ..."},
    "raw_uri": "s3://bucket/path/to/raw-output.txt",
    "debug_snippet": "optional short snippet for debugging (gated by env flag, max 500 chars)"
  },

  "error": {
    "code": "string",
    "message": "string",
    "details": {"...": "..."}
  }
}
```

Rules:
- If `status=succeeded`, `result.content` MUST be valid JSON that conforms to the requested schema.
- If `status=failed`, `error` MUST be populated. `result` may be omitted.
- `raw_uri` and `debug_snippet` must be disabled by default and only enabled for debugging (avoid leaking sensitive OCR text).

## API Endpoints

### LLM Proxy
- `POST /internal/queue/enqueue`
  - Auth: service-to-service token (Jarvis Auth) or shared secret header
  - Validates payload (required: `job_id`, `job_type`, `request.messages`, and if `response_format.type=json_object` then `json_schema`)
  - Idempotent: repeated enqueue calls with the same `job_id`/`idempotency_key` must not create duplicate queued work
  - Should return the existing accepted response for duplicates: `{ "accepted": true, "job_id": "...", "deduped": true }`
  - Enqueues into llm-proxy’s internal Redis queue
  - Returns: `{ "accepted": true, "job_id": "..." }`

### Recipes Server
- `POST /internal/llm/callback`
  - Auth: service-to-service token (Jarvis Auth) or shared secret header
  - Idempotent: repeated callbacks for same `job_id` must not corrupt state
  - Updates recipe parsing job status/result in the Recipes job store
  - Returns 200 OK quickly (no heavy processing)

## Cancellation & Expiry
- Jobs must include `ttl_seconds` (or an equivalent `expires_at`).
- LLM Proxy should drop/mark jobs as failed when expired.
- Optional (nice-to-have):
  - `POST /internal/queue/cancel/{job_id}` to cancel queued/in-flight work (best-effort).
  - If canceled, LLM Proxy should callback with `status=failed` and `error.code="canceled"`.

## Reliability & Retries

### Enqueue call (Recipes -> LLM Proxy)
- Retry on 5xx / network errors with exponential backoff (e.g., 1s, 2s, 4s, max 30s)
- If enqueue ultimately fails, Recipes job should transition to `failed` with a clear error message

### Processing retries (inside LLM Proxy)
- Attempt budget configurable via env (default 3)
- Backoff between attempts
- If JSON validation fails repeatedly, mark job `failed` and include last parse/validation error in callback

### Callback delivery (LLM Proxy -> Recipes)
- LLM Proxy must retry callback on non-2xx responses
- Use idempotency key: `job_id`
- Store callback payload in llm-proxy for a short retention window (e.g., 24h) for replay/debug
- After a configurable max retry window (e.g., 15 minutes or `ttl_seconds`), mark as `callback_failed` and stop retrying.
- Store the last callback attempt status and a replay handle (admin-only) for manual replay.

## Security
- All internal endpoints must require authentication.
- Recommended: use Jarvis Auth service-to-service token with audience restrictions.
- Minimum viable: shared secret header (e.g., `X-Jarvis-Internal-Token`) plus IP allowlist.

## Observability
- `trace_id` should be propagated from Recipes -> LLM Proxy -> Recipes callback.
- Both services log:
  - job_id, idempotency_key, job_type, job_type_version, trace_id, model, schema hash, attempt count, timing
- Metrics:
  - queue depth, time-in-queue, processing time, success rate, retry counts

## Acceptance Criteria
- No synchronous `chat/completions` calls from Recipes parsing path.
- Recipes parsing job endpoint never times out waiting on llm output.
- LLM Proxy validates schema when `response_format.type=json_object` and `json_schema` is provided.
- Callback updates job status correctly and is idempotent.
- End-to-end: recipe parse completes successfully under invalid-json scenarios without HTTP timeout.


## Environment Configuration

### Shared Redis Configuration
Both Recipes Server and LLM Proxy use Redis for internal queues.
Each service owns its own queues and keyspace.

Required:
- REDIS_HOST (default: localhost)
- REDIS_PORT (default: 6379)

Optional:
- REDIS_DB (default: 0)
- REDIS_PASSWORD
- REDIS_URL (if set, overrides host/port/db/password)

Notes:
- Services must not write to each other’s queues.
- Queue names must be namespaced per service.
- On startup, each service should log resolved Redis connection info
  (excluding secrets).