# Jarvis LLM Proxy API Environment Configuration Template
# Copy this file to .env and configure according to your setup

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

# Main model configuration
JARVIS_MODEL_NAME=.models/Llama-3.2-3B-Instruct-Q4_0_4_4.gguf
JARVIS_MODEL_BACKEND=GGUF
JARVIS_MODEL_CHAT_FORMAT=llama3
JARVIS_MODEL_CONTEXT_WINDOW=8192

# Lightweight model configuration (for fast operations)
JARVIS_LIGHTWEIGHT_MODEL_NAME=.models/Llama-3.2-3B-Instruct-Q4_0_4_4.gguf
JARVIS_LIGHTWEIGHT_MODEL_BACKEND=GGUF
JARVIS_LIGHTWEIGHT_MODEL_CHAT_FORMAT=llama3
JARVIS_LIGHTWEIGHT_MODEL_CONTEXT_WINDOW=8192

# Disable LLM date key extractor (set to true if date extraction is merged into main model)
JARVIS_DISABLE_DATE_KEY_LLM=false

# Vision model configuration (vision-capable backend required)
# Supported backends: MOCK, MLX, MLX_VISION, TRANSFORMERS, VLLM, REST, GGUF, GGUF_VISION
JARVIS_VISION_MODEL_NAME=jarvis-vision-11b
JARVIS_VISION_MODEL_BACKEND=MOCK
JARVIS_VISION_MODEL_CHAT_FORMAT=qwen
JARVIS_VISION_MODEL_CONTEXT_WINDOW=8192

# Vision-specific vLLM quantization (optional, falls back to JARVIS_VLLM_QUANTIZATION)
# Use "awq" for AWQ-quantized models like Qwen2.5-VL-7B-Instruct-AWQ
JARVIS_VISION_VLLM_QUANTIZATION=

# GGUF Vision settings (for LLaVA-style models)
# Required when using GGUF/GGUF_VISION backend - path to CLIP/mmproj GGUF file
JARVIS_VISION_CLIP_MODEL_PATH=
# GPU layers for vision model (0=CPU only, -1=all on GPU)
JARVIS_VISION_N_GPU_LAYERS=0

# ============================================================================
# INFERENCE ENGINE SELECTION
# ============================================================================

# Choose your inference engine:
# - "llama_cpp" (default): llama.cpp with hardware acceleration
# - "vllm": High-performance inference engine (GPU recommended)
# - "transformers": Use with TRANSFORMERS backend for native HF models
JARVIS_INFERENCE_ENGINE=llama_cpp

# ============================================================================
# HARDWARE ACCELERATION SETTINGS
# ============================================================================

# GPU acceleration settings (automatically configured by setup.sh)
# Uncomment and adjust based on your hardware:

# For NVIDIA GPUs (CUDA):
# JARVIS_N_GPU_LAYERS=-1                    # -1 = all layers on GPU, 0 = CPU only
# JARVIS_N_BATCH=1024                       # Batch size for processing
# JARVIS_N_UBATCH=1024                      # Micro-batch size

# For Apple Silicon (Metal):
# JARVIS_N_GPU_LAYERS=-1                    # -1 = all layers on GPU
# JARVIS_N_BATCH=512                        # Smaller batch for Metal
# JARVIS_N_UBATCH=512
# LLAMA_METAL=true                          # Enable Metal acceleration

# For AMD GPUs (ROCm - experimental):
# JARVIS_N_GPU_LAYERS=-1                    # -1 = all layers on GPU
# JARVIS_N_BATCH=512
# JARVIS_N_UBATCH=512

# For CPU only:
# JARVIS_N_GPU_LAYERS=0                     # Force CPU usage
# JARVIS_N_BATCH=256                        # Smaller batch for CPU
# JARVIS_N_UBATCH=256
# LLAMA_METAL=false

# Current configuration (adjust based on your setup):
JARVIS_N_GPU_LAYERS=-1
JARVIS_N_BATCH=1024
JARVIS_N_UBATCH=1024
JARVIS_N_THREADS=10
LLAMA_METAL=false

# ============================================================================
# VLLM-SPECIFIC SETTINGS (only used when JARVIS_INFERENCE_ENGINE=vllm)
# ============================================================================

# vLLM tensor parallelism (number of GPUs to use)
JARVIS_VLLM_TENSOR_PARALLEL_SIZE=1

# GPU memory utilization (0.0 to 1.0)
JARVIS_VLLM_GPU_MEMORY_UTILIZATION=0.9

# Note: vLLM uses JARVIS_MODEL_CONTEXT_WINDOW and JARVIS_LIGHTWEIGHT_MODEL_CONTEXT_WINDOW
# for max model length instead of a separate vLLM-specific setting

# vLLM Performance Tuning (to reduce latency spikes)
JARVIS_VLLM_MAX_BATCHED_TOKENS=8192
JARVIS_VLLM_MAX_NUM_SEQS=256

# vLLM Quantization (optional)
JARVIS_VLLM_QUANTIZATION=                   # e.g. awq, gptq, fp8

# ============================================================================
# TRANSFORMERS BACKEND SETTINGS (only used when JARVIS_MODEL_BACKEND=TRANSFORMERS)
# ============================================================================

# Device configuration
JARVIS_DEVICE=auto                          # auto, cuda, mps, cpu
JARVIS_TORCH_DTYPE=auto                     # auto, float16, float32, bfloat16

# Quantization (requires bitsandbytes)
JARVIS_USE_QUANTIZATION=false               # Enable quantization
JARVIS_QUANTIZATION_TYPE=4bit               # 4bit, 8bit

# Generation parameters
JARVIS_TOP_K=50                             # Top-K sampling
JARVIS_REPETITION_PENALTY=1.1               # Repetition penalty
JARVIS_DO_SAMPLE=true                       # Enable sampling

# Memory optimization
JARVIS_USE_CACHE=true                       # Enable model caching
JARVIS_TRUST_REMOTE_CODE=false              # Trust remote code execution

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================

# Memory and caching
JARVIS_ENABLE_CONTEXT_CACHE=true
JARVIS_MAX_CACHE_SIZE=200
JARVIS_MAX_TOKENS=8000

# ============================================================================
# ADAPTER TRAINING (llm-proxy worker)
# ============================================================================

# Training command to execute. The command receives env vars:
# JARVIS_TRAIN_JOB_ID, JARVIS_TRAIN_NODE_ID, JARVIS_TRAIN_BASE_MODEL_ID,
# JARVIS_TRAIN_DATASET_HASH, JARVIS_TRAIN_DATASET_PATH, JARVIS_TRAIN_PARAMS_PATH,
# JARVIS_TRAIN_OUTPUT_DIR
JARVIS_ADAPTER_TRAIN_CMD=python3 scripts/train_adapter.py

# Adapter storage directory (flat structure: {dir}/{hash}/)
# Used by both training output and runtime loading
LLM_PROXY_ADAPTER_DIR=/tmp/jarvis-adapters

# Optional public URL prefix for artifacts (defaults to file:// paths)
JARVIS_ADAPTER_PUBLIC_URL_PREFIX=

# Training timeout (seconds). Defaults to job ttl if unset.
JARVIS_ADAPTER_TRAIN_TIMEOUT_SECONDS=

# Training defaults (overridden by request.params)
JARVIS_ADAPTER_MAX_SEQ_LEN=2048
JARVIS_ADAPTER_BATCH_SIZE=1
JARVIS_ADAPTER_GRAD_ACCUM=4
JARVIS_ADAPTER_EPOCHS=1
JARVIS_ADAPTER_LEARNING_RATE=2e-4
JARVIS_ADAPTER_LORA_R=16
JARVIS_ADAPTER_LORA_ALPHA=32
JARVIS_ADAPTER_LORA_DROPOUT=0.05
JARVIS_ADAPTER_TRAIN_DTYPE=auto           # auto, bf16, fp16
JARVIS_ADAPTER_TRAIN_LOAD_IN_4BIT=false   # BnB 4-bit for node adapter training (disable for AWQ models)
JARVIS_ADAPTER_TRAIN_LOAD_IN_8BIT=false   # BnB 8-bit for node adapter training

# Date key adapter training (scripts/train_date_adapter.py)
# These are separate from node adapter training since they use different base models
JARVIS_DATE_ADAPTER_TRAIN_LOAD_IN_4BIT=true  # BnB 4-bit (safe for unquantized lightweight models)

# HuggingFace base model ID (used for training when base_model_id is .gguf)
# For training adapters, this maps a GGUF model to its HF equivalent.
JARVIS_ADAPTER_HF_BASE_MODEL_ID=

# NOTE: GGUF LoRA conversion is disabled due to llama.cpp runtime issues.
# Adapters are produced in PEFT format only and should be used with
# the vLLM or Transformers backend.
# JARVIS_ADAPTER_GGUF_CONVERT_CMD is no longer used.

# vLLM LoRA settings (for dynamic per-request adapter loading)
JARVIS_VLLM_MAX_LORA_RANK=64
JARVIS_VLLM_MAX_LORAS=1

# ============================================================================
# S3/MINIO ADAPTER STORAGE (for per-node dynamic adapter loading)
# ============================================================================

# S3-compatible endpoint (required for MinIO, omit for AWS S3)
S3_ENDPOINT_URL=http://localhost:9000

# S3 path style (true for MinIO, false for AWS S3)
S3_FORCE_PATH_STYLE=true

# S3 region (default: us-east-1)
S3_REGION=us-east-1

# S3 credentials
AWS_ACCESS_KEY_ID=minioadmin
AWS_SECRET_ACCESS_KEY=minioadmin

# S3 adapter bucket and prefix (for remote storage)
# Structure: s3://{bucket}/{prefix}/{hash}/adapter.zip
LLM_PROXY_ADAPTER_BUCKET=jarvis-llm-proxy
LLM_PROXY_ADAPTER_PREFIX=adapters

# Note: LLM_PROXY_ADAPTER_DIR (above) is used for local storage
# Adapters are stored flat by hash: {LLM_PROXY_ADAPTER_DIR}/{hash}/

# Logging level (info, debug, warning, error)
LLAMA_LOG_LEVEL=info

# Transformers device map (auto|none). "auto" enables accelerate offload on CUDA.
JARVIS_TRANSFORMERS_DEVICE_MAP=auto

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================

# Server settings
SERVER_HOST=0.0.0.0
SERVER_PORT=8000

# Debug settings
DEBUG=false
DEBUG_PORT=5678

# ============================================================================
# AUTHENTICATION
# ============================================================================

# Internal token for service-to-service communication (model-service <-> API)
LLM_PROXY_INTERNAL_TOKEN=

# Admin token for human operators (used by patch_settings.py and admin endpoints)
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
JARVIS_ADMIN_TOKEN=

# ============================================================================
# REST BACKEND CONFIGURATION (if using REST backend)
# ============================================================================

# Uncomment and configure if using REST backend instead of local models
# JARVIS_MODEL_BACKEND=REST
# JARVIS_REST_MODEL_URL=https://api.openai.com
# JARVIS_REST_PROVIDER=openai
# JARVIS_REST_AUTH_TYPE=bearer
# JARVIS_REST_AUTH_TOKEN=your_api_key_here
# JARVIS_REST_REQUEST_FORMAT=openai

# ============================================================================
# TESTING CONFIGURATION
# ============================================================================

TEST_BASE_URL=http://localhost:8000

# ============================================================================
# PERFORMANCE PRESETS
# ============================================================================

# Uncomment one of the presets below and comment out individual settings above

# PRESET: Fast & Lightweight (3B model)
# JARVIS_MODEL_NAME=.models/Llama-3.2-3B-Instruct-Q4_K_M.gguf
# JARVIS_MODEL_CONTEXT_WINDOW=8192
# JARVIS_N_BATCH=1024
# JARVIS_N_UBATCH=1024
# JARVIS_MAX_CACHE_SIZE=300
# JARVIS_MAX_TOKENS=8000

# PRESET: Balanced (8B model)
# JARVIS_MODEL_NAME=.models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
# JARVIS_MODEL_CONTEXT_WINDOW=6144
# JARVIS_N_BATCH=512
# JARVIS_N_UBATCH=512
# JARVIS_MAX_CACHE_SIZE=100
# JARVIS_MAX_TOKENS=6000

# PRESET: High Quality (14B+ model)
# JARVIS_MODEL_NAME=.models/Qwen2.5-14B-Instruct-Q4_K_M.gguf
# JARVIS_MODEL_CONTEXT_WINDOW=4096
# JARVIS_N_BATCH=256
# JARVIS_N_UBATCH=256
# JARVIS_MAX_CACHE_SIZE=50
# JARVIS_MAX_TOKENS=4000

# PRESET: Conservative (low memory)
# JARVIS_MODEL_CONTEXT_WINDOW=2048
# JARVIS_N_BATCH=128
# JARVIS_N_UBATCH=128
# JARVIS_MAX_CACHE_SIZE=25
# JARVIS_MAX_TOKENS=2000

# ============================================================================
# PRESET: Llama-3.2-3B-Instruct with vLLM on NVIDIA 3080 Ti (12GB VRAM)
# ============================================================================
# Optimized for adapter-enabled inference with PEFT LoRA
# Model: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct
#
# VRAM breakdown (~12GB 3080 Ti):
#   - Model weights (bf16): ~6GB
#   - KV cache (4K context): ~1-2GB
#   - LoRA adapters: ~100-500MB each
#   - CUDA overhead: ~1-2GB
#   - Total: ~9-10GB (leaves headroom)
#
# To use this preset, copy these values to your .env file:
#
# JARVIS_MODEL_NAME=meta-llama/Llama-3.2-3B-Instruct
# JARVIS_MODEL_BACKEND=TRANSFORMERS
# JARVIS_MODEL_CHAT_FORMAT=llama3
# JARVIS_MODEL_CONTEXT_WINDOW=4096
# JARVIS_INFERENCE_ENGINE=vllm
#
# # vLLM optimization for 3080 Ti
# JARVIS_VLLM_GPU_MEMORY_UTILIZATION=0.85
# JARVIS_VLLM_MAX_BATCHED_TOKENS=4096
# JARVIS_VLLM_MAX_NUM_SEQS=64
# JARVIS_VLLM_TENSOR_PARALLEL_SIZE=1
#
# # LoRA adapter support
# JARVIS_VLLM_MAX_LORA_RANK=64
# JARVIS_VLLM_MAX_LORAS=1
#
# # Generation settings
# JARVIS_MAX_TOKENS=2048
# JARVIS_TOP_P=0.95
# JARVIS_TOP_K=50
# JARVIS_REPETITION_PENALTY=1.1
#
# # HuggingFace authentication (required for gated models like Llama 3.2)
# # Get your token at: https://huggingface.co/settings/tokens
# HUGGINGFACE_HUB_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#
# # Adapter training (uses the same HF model for training)
# JARVIS_ADAPTER_HF_BASE_MODEL_ID=meta-llama/Llama-3.2-3B-Instruct
# JARVIS_ADAPTER_TRAIN_LOAD_IN_4BIT=true
# JARVIS_ADAPTER_TRAIN_DTYPE=bf16
# JARVIS_ADAPTER_BATCH_SIZE=1
# JARVIS_ADAPTER_GRAD_ACCUM=8
# # Device map for training: "0" (default, single GPU), "auto", "1", or JSON for multi-GPU
# # Examples: "0" -> GPU 0, "auto" -> automatic, '{"": 0}' -> explicit single GPU
# JARVIS_ADAPTER_TRAIN_DEVICE_MAP=0

# ============================================================================
# PRESET: Llama-3.2-3B-Instruct with Transformers (lower VRAM alternative)
# ============================================================================
# Use this if vLLM has issues or you want more memory headroom
#
# JARVIS_MODEL_NAME=meta-llama/Llama-3.2-3B-Instruct
# JARVIS_MODEL_BACKEND=TRANSFORMERS
# JARVIS_MODEL_CHAT_FORMAT=llama3
# JARVIS_MODEL_CONTEXT_WINDOW=4096
# JARVIS_INFERENCE_ENGINE=transformers
#
# # Device and dtype
# JARVIS_DEVICE=cuda
# JARVIS_TORCH_DTYPE=bfloat16
# JARVIS_TRANSFORMERS_DEVICE_MAP=auto
#
# # Optional: Enable 4-bit quantization (~2GB VRAM for model)
# JARVIS_USE_QUANTIZATION=true
# JARVIS_QUANTIZATION_TYPE=4bit
#
# # Generation settings
# JARVIS_MAX_TOKENS=2048
# JARVIS_TOP_P=0.95
# JARVIS_TOP_K=50
# JARVIS_REPETITION_PENALTY=1.1
# JARVIS_DO_SAMPLE=true
#
# # HuggingFace authentication
# HUGGINGFACE_HUB_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# ============================================================================
# PRESET: Qwen2.5-VL-7B-Instruct-AWQ Vision Model (OCR/Document Understanding)
# ============================================================================
# High-quality vision model for OCR, document parsing, and image understanding.
# Uses AWQ 4-bit quantization to fit in ~7GB VRAM on 3080 Ti (12GB).
#
# This model should be loaded on-demand (swap pattern) since it can't run
# simultaneously with the main model on a single 12GB GPU.
#
# VRAM breakdown (~12GB 3080 Ti):
#   - Model weights (AWQ 4-bit): ~7GB
#   - Vision encoder: ~0.5GB
#   - KV cache (8K context): ~1GB
#   - CUDA overhead: ~1-2GB
#   - Total: ~9-11GB
#
# To use this preset, copy these values to your .env file:
#
# JARVIS_VISION_MODEL_NAME=Qwen/Qwen2.5-VL-7B-Instruct-AWQ
# JARVIS_VISION_MODEL_BACKEND=VLLM
# JARVIS_VISION_MODEL_CHAT_FORMAT=qwen
# JARVIS_VISION_MODEL_CONTEXT_WINDOW=8192
# JARVIS_VISION_VLLM_QUANTIZATION=awq
#
# Or for a local model path:
# JARVIS_VISION_MODEL_NAME=.models/Qwen2.5-VL-7B-Instruct-AWQ
#
# Note: The main model must be unloaded before loading the vision model.
# This is handled automatically by the queue worker for vision requests.

# ============================================================================
# PRESET: LLaVA-1.5 GGUF Vision Model (CPU-based, saves GPU memory)
# ============================================================================
# Quantized LLaVA model for vision tasks running on CPU while main model uses GPU.
# This allows running both text and vision models without GPU memory conflicts.
#
# Download models:
#   # Main LLaVA model (pick a quantization level based on your RAM)
#   wget https://huggingface.co/mys/ggml_llava-v1.5-7b/resolve/main/ggml-model-q4_k.gguf
#
#   # CLIP vision encoder (required for image processing)
#   wget https://huggingface.co/mys/ggml_llava-v1.5-7b/resolve/main/mmproj-model-f16.gguf
#
# Memory requirements:
#   - LLaVA 7B Q4_K: ~4GB RAM
#   - CLIP encoder: ~600MB RAM
#   - Total: ~5GB system RAM (GPU free for main model)
#
# To use this preset, copy these values to your .env file:
#
# JARVIS_VISION_MODEL_NAME=.models/ggml-model-q4_k.gguf
# JARVIS_VISION_MODEL_BACKEND=GGUF_VISION
# JARVIS_VISION_CLIP_MODEL_PATH=.models/mmproj-model-f16.gguf
# JARVIS_VISION_N_GPU_LAYERS=0
# JARVIS_VISION_MODEL_CONTEXT_WINDOW=4096
#
# Note: GGUF vision runs synchronously (not on-demand swap pattern like vLLM).
# It loads at startup and stays in memory.
