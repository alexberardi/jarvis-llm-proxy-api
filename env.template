# Jarvis LLM Proxy API Environment Configuration Template
# Copy this file to .env and configure according to your setup

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

# Main model configuration
JARVIS_MODEL_NAME=.models/Llama-3.2-3B-Instruct-Q4_0_4_4.gguf
JARVIS_MODEL_BACKEND=GGUF
JARVIS_MODEL_CHAT_FORMAT=llama3
JARVIS_MODEL_CONTEXT_WINDOW=8192

# Lightweight model configuration (for fast operations)
JARVIS_LIGHTWEIGHT_MODEL_NAME=.models/Llama-3.2-3B-Instruct-Q4_0_4_4.gguf
JARVIS_LIGHTWEIGHT_MODEL_BACKEND=GGUF
JARVIS_LIGHTWEIGHT_MODEL_CHAT_FORMAT=llama3
JARVIS_LIGHTWEIGHT_MODEL_CONTEXT_WINDOW=8192

# ============================================================================
# INFERENCE ENGINE SELECTION
# ============================================================================

# Choose your inference engine:
# - "llama_cpp" (default): llama.cpp with hardware acceleration
# - "vllm": High-performance inference engine (GPU recommended)
# - "transformers": Use with TRANSFORMERS backend for native HF models
JARVIS_INFERENCE_ENGINE=llama_cpp

# ============================================================================
# HARDWARE ACCELERATION SETTINGS
# ============================================================================

# GPU acceleration settings (automatically configured by setup.sh)
# Uncomment and adjust based on your hardware:

# For NVIDIA GPUs (CUDA):
# JARVIS_N_GPU_LAYERS=-1                    # -1 = all layers on GPU, 0 = CPU only
# JARVIS_N_BATCH=1024                       # Batch size for processing
# JARVIS_N_UBATCH=1024                      # Micro-batch size

# For Apple Silicon (Metal):
# JARVIS_N_GPU_LAYERS=-1                    # -1 = all layers on GPU
# JARVIS_N_BATCH=512                        # Smaller batch for Metal
# JARVIS_N_UBATCH=512
# LLAMA_METAL=true                          # Enable Metal acceleration

# For AMD GPUs (ROCm - experimental):
# JARVIS_N_GPU_LAYERS=-1                    # -1 = all layers on GPU
# JARVIS_N_BATCH=512
# JARVIS_N_UBATCH=512

# For CPU only:
# JARVIS_N_GPU_LAYERS=0                     # Force CPU usage
# JARVIS_N_BATCH=256                        # Smaller batch for CPU
# JARVIS_N_UBATCH=256
# LLAMA_METAL=false

# Current configuration (adjust based on your setup):
JARVIS_N_GPU_LAYERS=-1
JARVIS_N_BATCH=1024
JARVIS_N_UBATCH=1024
JARVIS_N_THREADS=10
LLAMA_METAL=false

# ============================================================================
# VLLM-SPECIFIC SETTINGS (only used when JARVIS_INFERENCE_ENGINE=vllm)
# ============================================================================

# vLLM tensor parallelism (number of GPUs to use)
JARVIS_VLLM_TENSOR_PARALLEL_SIZE=1

# GPU memory utilization (0.0 to 1.0)
JARVIS_VLLM_GPU_MEMORY_UTILIZATION=0.9

# Note: vLLM uses JARVIS_MODEL_CONTEXT_WINDOW and JARVIS_LIGHTWEIGHT_MODEL_CONTEXT_WINDOW
# for max model length instead of a separate vLLM-specific setting

# vLLM Performance Tuning (to reduce latency spikes)
JARVIS_VLLM_MAX_BATCHED_TOKENS=8192
JARVIS_VLLM_MAX_NUM_SEQS=256

# ============================================================================
# TRANSFORMERS BACKEND SETTINGS (only used when JARVIS_MODEL_BACKEND=TRANSFORMERS)
# ============================================================================

# Device configuration
JARVIS_DEVICE=auto                          # auto, cuda, mps, cpu
JARVIS_TORCH_DTYPE=auto                     # auto, float16, float32, bfloat16

# Quantization (requires bitsandbytes)
JARVIS_USE_QUANTIZATION=false               # Enable quantization
JARVIS_QUANTIZATION_TYPE=4bit               # 4bit, 8bit

# Generation parameters
JARVIS_TOP_K=50                             # Top-K sampling
JARVIS_REPETITION_PENALTY=1.1               # Repetition penalty
JARVIS_DO_SAMPLE=true                       # Enable sampling

# Memory optimization
JARVIS_USE_CACHE=true                       # Enable model caching
JARVIS_TRUST_REMOTE_CODE=false              # Trust remote code execution

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================

# Memory and caching
JARVIS_ENABLE_CONTEXT_CACHE=true
JARVIS_MAX_CACHE_SIZE=200
JARVIS_MAX_TOKENS=8000

# Logging level (info, debug, warning, error)
LLAMA_LOG_LEVEL=info

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================

# Server settings
SERVER_HOST=0.0.0.0
SERVER_PORT=8000

# Debug settings
DEBUG=false
DEBUG_PORT=5678

# ============================================================================
# REST BACKEND CONFIGURATION (if using REST backend)
# ============================================================================

# Uncomment and configure if using REST backend instead of local models
# JARVIS_MODEL_BACKEND=REST
# JARVIS_REST_MODEL_URL=https://api.openai.com
# JARVIS_REST_PROVIDER=openai
# JARVIS_REST_AUTH_TYPE=bearer
# JARVIS_REST_AUTH_TOKEN=your_api_key_here
# JARVIS_REST_REQUEST_FORMAT=openai

# ============================================================================
# TESTING CONFIGURATION
# ============================================================================

TEST_BASE_URL=http://localhost:8000

# ============================================================================
# PERFORMANCE PRESETS
# ============================================================================

# Uncomment one of the presets below and comment out individual settings above

# PRESET: Fast & Lightweight (3B model)
# JARVIS_MODEL_NAME=.models/Llama-3.2-3B-Instruct-Q4_K_M.gguf
# JARVIS_MODEL_CONTEXT_WINDOW=8192
# JARVIS_N_BATCH=1024
# JARVIS_N_UBATCH=1024
# JARVIS_MAX_CACHE_SIZE=300
# JARVIS_MAX_TOKENS=8000

# PRESET: Balanced (8B model)
# JARVIS_MODEL_NAME=.models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
# JARVIS_MODEL_CONTEXT_WINDOW=6144
# JARVIS_N_BATCH=512
# JARVIS_N_UBATCH=512
# JARVIS_MAX_CACHE_SIZE=100
# JARVIS_MAX_TOKENS=6000

# PRESET: High Quality (14B+ model)
# JARVIS_MODEL_NAME=.models/Qwen2.5-14B-Instruct-Q4_K_M.gguf
# JARVIS_MODEL_CONTEXT_WINDOW=4096
# JARVIS_N_BATCH=256
# JARVIS_N_UBATCH=256
# JARVIS_MAX_CACHE_SIZE=50
# JARVIS_MAX_TOKENS=4000

# PRESET: Conservative (low memory)
# JARVIS_MODEL_CONTEXT_WINDOW=2048
# JARVIS_N_BATCH=128
# JARVIS_N_UBATCH=128
# JARVIS_MAX_CACHE_SIZE=25
# JARVIS_MAX_TOKENS=2000
