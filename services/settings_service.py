"""Settings service for jarvis-llm-proxy-api.

This module provides the settings service using the shared jarvis-settings-client library.
It defines LLM-proxy-specific settings and provides convenience methods for accessing
model and inference configurations.
"""

import logging

from jarvis_settings_client import SettingDefinition, SettingsService

logger = logging.getLogger("uvicorn")


# All settings definitions with their categories, types, and env fallbacks
SETTINGS_DEFINITIONS: list[SettingDefinition] = [
    # ==================== model.main ====================
    SettingDefinition(
        key="model.main.name",
        category="model.main",
        value_type="string",
        default=".models/Llama-3.2-3B-Instruct-Q4_0_4_4.gguf",
        description="Main model path or HuggingFace ID",
        env_fallback="JARVIS_MODEL_NAME",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.main.backend",
        category="model.main",
        value_type="string",
        default="GGUF",
        description="Main model backend: GGUF, VLLM, TRANSFORMERS, REST, MOCK",
        env_fallback="JARVIS_MODEL_BACKEND",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.main.chat_format",
        category="model.main",
        value_type="string",
        default="llama3",
        description="Chat template format: llama3, chatml, mistral, etc.",
        env_fallback="JARVIS_MODEL_CHAT_FORMAT",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.main.context_window",
        category="model.main",
        value_type="int",
        default=8192,
        description="Maximum context window size in tokens",
        env_fallback="JARVIS_MODEL_CONTEXT_WINDOW",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.main.stop_tokens",
        category="model.main",
        value_type="string",
        default="",
        description="Comma-separated stop tokens",
        env_fallback="JARVIS_MODEL_STOP_TOKENS",
        requires_reload=True,
    ),
    # ==================== model.lightweight ====================
    SettingDefinition(
        key="model.lightweight.name",
        category="model.lightweight",
        value_type="string",
        default="",
        description="Lightweight model path (empty to share main model)",
        env_fallback="JARVIS_LIGHTWEIGHT_MODEL_NAME",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.lightweight.backend",
        category="model.lightweight",
        value_type="string",
        default="",
        description="Lightweight model backend",
        env_fallback="JARVIS_LIGHTWEIGHT_MODEL_BACKEND",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.lightweight.chat_format",
        category="model.lightweight",
        value_type="string",
        default="",
        description="Lightweight model chat format",
        env_fallback="JARVIS_LIGHTWEIGHT_MODEL_CHAT_FORMAT",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.lightweight.context_window",
        category="model.lightweight",
        value_type="int",
        default=8192,
        description="Lightweight model context window",
        env_fallback="JARVIS_LIGHTWEIGHT_MODEL_CONTEXT_WINDOW",
        requires_reload=True,
    ),
    # ==================== model.vision ====================
    SettingDefinition(
        key="model.vision.name",
        category="model.vision",
        value_type="string",
        default="",
        description="Vision model path or HuggingFace ID",
        env_fallback="JARVIS_VISION_MODEL_NAME",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.vision.backend",
        category="model.vision",
        value_type="string",
        default="",
        description="Vision model backend",
        env_fallback="JARVIS_VISION_MODEL_BACKEND",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.vision.context_window",
        category="model.vision",
        value_type="int",
        default=131072,
        description="Vision model context window",
        env_fallback="JARVIS_VISION_MODEL_CONTEXT_WINDOW",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.vision.chat_format",
        category="model.vision",
        value_type="string",
        default="qwen",
        description="Vision model chat format (e.g., qwen, chatml)",
        env_fallback="JARVIS_VISION_MODEL_CHAT_FORMAT",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.vision.vllm_quantization",
        category="model.vision",
        value_type="string",
        default="",
        description="vLLM quantization for vision model (e.g., awq, gptq)",
        env_fallback="JARVIS_VISION_VLLM_QUANTIZATION",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.vision.clip_model_path",
        category="model.vision",
        value_type="string",
        default="",
        description="Path to CLIP/mmproj.gguf file for GGUF vision models (LLaVA-style)",
        env_fallback="JARVIS_VISION_CLIP_MODEL_PATH",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.vision.n_gpu_layers",
        category="model.vision",
        value_type="int",
        default=0,
        description="Number of layers to offload to GPU for GGUF vision models (0=CPU only, -1=all)",
        env_fallback="JARVIS_VISION_N_GPU_LAYERS",
        requires_reload=True,
    ),
    # ==================== model.cloud ====================
    SettingDefinition(
        key="model.cloud.name",
        category="model.cloud",
        value_type="string",
        default="",
        description="Cloud model identifier",
        env_fallback="JARVIS_CLOUD_MODEL_NAME",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.cloud.backend",
        category="model.cloud",
        value_type="string",
        default="REST",
        description="Cloud model backend (typically REST)",
        env_fallback="JARVIS_CLOUD_MODEL_BACKEND",
        requires_reload=True,
    ),
    SettingDefinition(
        key="model.cloud.context_window",
        category="model.cloud",
        value_type="int",
        default=4096,
        description="Cloud model context window",
        env_fallback="JARVIS_CLOUD_MODEL_CONTEXT_WINDOW",
        requires_reload=True,
    ),
    # ==================== inference.vllm ====================
    SettingDefinition(
        key="inference.vllm.gpu_memory_utilization",
        category="inference.vllm",
        value_type="float",
        default=0.9,
        description="GPU memory utilization (0.0-1.0)",
        env_fallback="JARVIS_VLLM_GPU_MEMORY_UTILIZATION",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.vllm.tensor_parallel_size",
        category="inference.vllm",
        value_type="int",
        default=1,
        description="Number of GPUs for tensor parallelism",
        env_fallback="JARVIS_VLLM_TENSOR_PARALLEL_SIZE",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.vllm.max_batched_tokens",
        category="inference.vllm",
        value_type="int",
        default=8192,
        description="Maximum batched tokens for vLLM",
        env_fallback="JARVIS_VLLM_MAX_BATCHED_TOKENS",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.vllm.max_num_seqs",
        category="inference.vllm",
        value_type="int",
        default=256,
        description="Maximum number of sequences",
        env_fallback="JARVIS_VLLM_MAX_NUM_SEQS",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.vllm.quantization",
        category="inference.vllm",
        value_type="string",
        default="",
        description="vLLM quantization: awq, gptq, fp8, or empty",
        env_fallback="JARVIS_VLLM_QUANTIZATION",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.vllm.max_lora_rank",
        category="inference.vllm",
        value_type="int",
        default=64,
        description="Maximum LoRA rank for adapters",
        env_fallback="JARVIS_VLLM_MAX_LORA_RANK",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.vllm.max_loras",
        category="inference.vllm",
        value_type="int",
        default=1,
        description="Maximum concurrent LoRA adapters",
        env_fallback="JARVIS_VLLM_MAX_LORAS",
        requires_reload=True,
    ),
    # ==================== inference.gguf ====================
    SettingDefinition(
        key="inference.gguf.n_gpu_layers",
        category="inference.gguf",
        value_type="int",
        default=-1,
        description="GPU layers (-1=all, 0=CPU only)",
        env_fallback="JARVIS_N_GPU_LAYERS",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.gguf.n_batch",
        category="inference.gguf",
        value_type="int",
        default=512,
        description="Batch size for llama.cpp",
        env_fallback="JARVIS_N_BATCH",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.gguf.n_ubatch",
        category="inference.gguf",
        value_type="int",
        default=512,
        description="Micro-batch size",
        env_fallback="JARVIS_N_UBATCH",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.gguf.n_threads",
        category="inference.gguf",
        value_type="int",
        default=10,
        description="Number of CPU threads",
        env_fallback="JARVIS_N_THREADS",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.gguf.flash_attn",
        category="inference.gguf",
        value_type="bool",
        default=True,
        description="Enable flash attention",
        env_fallback="JARVIS_FLASH_ATTN",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.gguf.f16_kv",
        category="inference.gguf",
        value_type="bool",
        default=True,
        description="Use FP16 for KV cache",
        env_fallback="JARVIS_F16_KV",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.gguf.mul_mat_q",
        category="inference.gguf",
        value_type="bool",
        default=True,
        description="Enable quantized matrix multiplication",
        env_fallback="JARVIS_MUL_MAT_Q",
        requires_reload=True,
    ),
    # ==================== inference.transformers ====================
    SettingDefinition(
        key="inference.transformers.device",
        category="inference.transformers",
        value_type="string",
        default="auto",
        description="Device: auto, cuda, mps, cpu",
        env_fallback="JARVIS_DEVICE",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.transformers.torch_dtype",
        category="inference.transformers",
        value_type="string",
        default="auto",
        description="Torch dtype: auto, float16, float32, bfloat16",
        env_fallback="JARVIS_TORCH_DTYPE",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.transformers.use_quantization",
        category="inference.transformers",
        value_type="bool",
        default=False,
        description="Enable bitsandbytes quantization",
        env_fallback="JARVIS_USE_QUANTIZATION",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.transformers.quantization_type",
        category="inference.transformers",
        value_type="string",
        default="4bit",
        description="Quantization type: 4bit, 8bit",
        env_fallback="JARVIS_QUANTIZATION_TYPE",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.transformers.device_map",
        category="inference.transformers",
        value_type="string",
        default="auto",
        description="Device map for transformers: auto, none",
        env_fallback="JARVIS_TRANSFORMERS_DEVICE_MAP",
        requires_reload=True,
    ),
    # ==================== inference.general ====================
    SettingDefinition(
        key="inference.general.engine",
        category="inference.general",
        value_type="string",
        default="llama_cpp",
        description="Default inference engine: llama_cpp, vllm, transformers",
        env_fallback="JARVIS_INFERENCE_ENGINE",
        requires_reload=True,
    ),
    SettingDefinition(
        key="inference.general.max_tokens",
        category="inference.general",
        value_type="int",
        default=7000,
        description="Default max generation tokens",
        env_fallback="JARVIS_MAX_TOKENS",
    ),
    SettingDefinition(
        key="inference.general.top_p",
        category="inference.general",
        value_type="float",
        default=0.95,
        description="Top-P sampling value",
        env_fallback="JARVIS_TOP_P",
    ),
    SettingDefinition(
        key="inference.general.top_k",
        category="inference.general",
        value_type="int",
        default=40,
        description="Top-K sampling value",
        env_fallback="JARVIS_TOP_K",
    ),
    SettingDefinition(
        key="inference.general.repeat_penalty",
        category="inference.general",
        value_type="float",
        default=1.1,
        description="Repetition penalty",
        env_fallback="JARVIS_REPEAT_PENALTY",
    ),
    # ==================== training ====================
    SettingDefinition(
        key="training.adapter_dir",
        category="training",
        value_type="string",
        default="/tmp/jarvis-adapters",
        description="Local adapter storage directory",
        env_fallback="LLM_PROXY_ADAPTER_DIR",
    ),
    SettingDefinition(
        key="training.batch_size",
        category="training",
        value_type="int",
        default=1,
        description="Training batch size",
        env_fallback="JARVIS_ADAPTER_BATCH_SIZE",
    ),
    SettingDefinition(
        key="training.grad_accum",
        category="training",
        value_type="int",
        default=4,
        description="Gradient accumulation steps",
        env_fallback="JARVIS_ADAPTER_GRAD_ACCUM",
    ),
    SettingDefinition(
        key="training.epochs",
        category="training",
        value_type="int",
        default=1,
        description="Training epochs",
        env_fallback="JARVIS_ADAPTER_EPOCHS",
    ),
    SettingDefinition(
        key="training.learning_rate",
        category="training",
        value_type="float",
        default=2e-4,
        description="Training learning rate",
        env_fallback="JARVIS_ADAPTER_LEARNING_RATE",
    ),
    SettingDefinition(
        key="training.lora_r",
        category="training",
        value_type="int",
        default=16,
        description="LoRA rank",
        env_fallback="JARVIS_ADAPTER_LORA_R",
    ),
    SettingDefinition(
        key="training.lora_alpha",
        category="training",
        value_type="int",
        default=32,
        description="LoRA alpha scaling",
        env_fallback="JARVIS_ADAPTER_LORA_ALPHA",
    ),
    SettingDefinition(
        key="training.lora_dropout",
        category="training",
        value_type="float",
        default=0.05,
        description="LoRA dropout rate",
        env_fallback="JARVIS_ADAPTER_LORA_DROPOUT",
    ),
    SettingDefinition(
        key="training.max_seq_len",
        category="training",
        value_type="int",
        default=2048,
        description="Maximum sequence length for training",
        env_fallback="JARVIS_ADAPTER_MAX_SEQ_LEN",
    ),
    # ==================== storage ====================
    SettingDefinition(
        key="storage.s3_endpoint_url",
        category="storage",
        value_type="string",
        default="",
        description="S3 endpoint URL (for MinIO)",
        env_fallback="S3_ENDPOINT_URL",
    ),
    SettingDefinition(
        key="storage.s3_region",
        category="storage",
        value_type="string",
        default="us-east-1",
        description="S3 region",
        env_fallback="S3_REGION",
    ),
    SettingDefinition(
        key="storage.adapter_bucket",
        category="storage",
        value_type="string",
        default="jarvis-llm-proxy",
        description="S3 bucket for adapters",
        env_fallback="LLM_PROXY_ADAPTER_BUCKET",
    ),
    SettingDefinition(
        key="storage.adapter_prefix",
        category="storage",
        value_type="string",
        default="adapters",
        description="S3 prefix for adapters",
        env_fallback="LLM_PROXY_ADAPTER_PREFIX",
    ),
    # ==================== logging ====================
    SettingDefinition(
        key="logging.console_level",
        category="logging",
        value_type="string",
        default="WARNING",
        description="Console log level",
        env_fallback="JARVIS_LOG_CONSOLE_LEVEL",
    ),
    SettingDefinition(
        key="logging.remote_level",
        category="logging",
        value_type="string",
        default="DEBUG",
        description="Remote (jarvis-logs) log level",
        env_fallback="JARVIS_LOG_REMOTE_LEVEL",
    ),
]


# Global singleton
_settings_service: SettingsService | None = None


def get_settings_service() -> SettingsService:
    """Get the global SettingsService instance."""
    global _settings_service
    if _settings_service is None:
        from db.models import Setting
        from db.session import SessionLocal

        _settings_service = SettingsService(
            definitions=SETTINGS_DEFINITIONS,
            get_db_session=SessionLocal,
            setting_model=Setting,
        )
    return _settings_service
