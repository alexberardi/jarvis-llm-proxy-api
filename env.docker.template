# ============================================================================
# Jarvis LLM Proxy API — Docker Environment Template
# ============================================================================
# Copy to .env and configure for your deployment:
#   cp env.docker.template .env
#
# This template is optimized for dockerized vLLM + CUDA deployment.
# For the full list of options, see env.template.
# ============================================================================

# ---- Model Configuration ---------------------------------------------------

JARVIS_MODEL_NAME=meta-llama/Llama-3.2-3B-Instruct
JARVIS_MODEL_BACKEND=TRANSFORMERS
JARVIS_MODEL_CHAT_FORMAT=llama3
JARVIS_MODEL_CONTEXT_WINDOW=4096

# Lightweight model (for fast operations like date key extraction)
JARVIS_LIGHTWEIGHT_MODEL_NAME=meta-llama/Llama-3.2-1B-Instruct
JARVIS_LIGHTWEIGHT_MODEL_BACKEND=TRANSFORMERS
JARVIS_LIGHTWEIGHT_MODEL_CHAT_FORMAT=llama3
JARVIS_LIGHTWEIGHT_MODEL_CONTEXT_WINDOW=4096

# Vision model (optional — uncomment to enable)
# JARVIS_VISION_MODEL_NAME=Qwen/Qwen2.5-VL-7B-Instruct-AWQ
# JARVIS_VISION_MODEL_BACKEND=VLLM
# JARVIS_VISION_MODEL_CHAT_FORMAT=qwen
# JARVIS_VISION_MODEL_CONTEXT_WINDOW=8192
# JARVIS_VISION_VLLM_QUANTIZATION=awq

# ---- Inference Engine -------------------------------------------------------

JARVIS_INFERENCE_ENGINE=vllm

# ---- vLLM Settings ----------------------------------------------------------

JARVIS_VLLM_TENSOR_PARALLEL_SIZE=1
JARVIS_VLLM_GPU_MEMORY_UTILIZATION=0.9
JARVIS_VLLM_MAX_BATCHED_TOKENS=8192
JARVIS_VLLM_MAX_NUM_SEQS=256
JARVIS_VLLM_QUANTIZATION=

# LoRA adapter support
JARVIS_VLLM_MAX_LORA_RANK=64
JARVIS_VLLM_MAX_LORAS=1

# ---- External Services (REQUIRED) ------------------------------------------
# Docker containers connect to host services via host.docker.internal

DATABASE_URL=postgresql://jarvis:jarvis@host.docker.internal:5432/llm_proxy
REDIS_URL=redis://host.docker.internal:6379/0

# ---- Server -----------------------------------------------------------------

SERVER_HOST=0.0.0.0
SERVER_PORT=8000
MODEL_SERVICE_PORT=8008

# Model service URL (docker networking — do not change for compose)
MODEL_SERVICE_URL=http://llm-proxy-model:8008

# ---- Authentication ---------------------------------------------------------

# Internal token for API <-> model service communication
LLM_PROXY_INTERNAL_TOKEN=
# Admin token for operator endpoints
JARVIS_ADMIN_TOKEN=

# HuggingFace token (required for gated models like Llama 3.2)
HUGGINGFACE_HUB_TOKEN=

# ---- Adapter Training -------------------------------------------------------

JARVIS_ADAPTER_TRAIN_CMD=python3 scripts/train_adapter.py
LLM_PROXY_ADAPTER_DIR=/tmp/jarvis-adapters
JARVIS_ADAPTER_HF_BASE_MODEL_ID=meta-llama/Llama-3.2-3B-Instruct

# ---- Logging ----------------------------------------------------------------

JARVIS_LOG_CONSOLE_LEVEL=INFO
JARVIS_LOG_REMOTE_LEVEL=DEBUG

# ---- Performance ------------------------------------------------------------

JARVIS_ENABLE_CONTEXT_CACHE=true
JARVIS_MAX_CACHE_SIZE=200
JARVIS_MAX_TOKENS=4096

# ---- Queue Worker -----------------------------------------------------------

RUN_QUEUE_WORKER=true
LLM_PROXY_QUEUE_NAME=llm_proxy_jobs
