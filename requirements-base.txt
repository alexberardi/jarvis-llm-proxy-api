# Base requirements for Jarvis LLM Proxy API
# These are installed on all platforms

fastapi
uvicorn[standard]
httpx
python-dotenv
sentencepiece==0.2.0
debugpy
Pillow
num2words
Pillow
redis>=5.0.0
rq>=1.15.1
psutil>=5.9.0  # For process management (vLLM cleanup)
sqlalchemy>=2.0.23
alembic>=1.12.1
psycopg2-binary>=2.9.9
boto3>=1.34.0  # S3/MinIO adapter storage

# Pin torch to match source-compiled vLLM (built 2026-01-26 against 2.9.1)
# DO NOT upgrade without recompiling vllm-src/ (~90 min build)
torch==2.9.1

# Adapter training (always uses transformers + PEFT regardless of inference backend)
peft>=0.12.0
accelerate>=0.21.0
bitsandbytes>=0.41.0  # Required for 4-bit quantized training
# Pin huggingface-hub to <1.0 to match transformers' requirement
huggingface-hub>=0.34.0,<1.0
# FastText for date key extraction (use wheel to avoid compilation issues)
fasttext-wheel>=0.9.2

# jarvis-log-client installed from local mount at runtime
# (provides centralized logging to jarvis-logs server)

# Platform-specific ML libraries are installed separately:
# - llama-cpp-python (with appropriate acceleration)
# - mlx-lm (macOS only)
# - vllm (optional, high-performance inference)
