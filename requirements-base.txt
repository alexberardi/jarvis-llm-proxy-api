# Base requirements for Jarvis LLM Proxy API
# These are installed on all platforms

fastapi
uvicorn[standard]
httpx
python-dotenv
sentencepiece==0.2.0
debugpy
Pillow
num2words
Pillow
redis>=5.0.0
rq>=1.15.1
psutil>=5.9.0  # For process management (vLLM cleanup)
sqlalchemy>=2.0.23
alembic>=1.12.1
psycopg2-binary>=2.9.9
boto3>=1.34.0  # S3/MinIO adapter storage

# torch is pinned by vLLM's own dependencies when using the vllm backend.
# For non-vLLM setups (GGUF, transformers-only), install torch separately.
torch>=2.9.1

# Adapter training (always uses transformers + PEFT regardless of inference backend)
transformers>=4.30.0
peft>=0.12.0
datasets>=2.14.0
trl>=0.7.0
accelerate>=0.21.0
bitsandbytes>=0.41.0  # Required for 4-bit quantized training
# huggingface-hub: mlx-lm requires >=1.0 (via transformers 5.x)
huggingface-hub>=0.34.0
# FastText for date key extraction (use wheel to avoid compilation issues)
fasttext-wheel>=0.9.2

# jarvis-log-client installed from local mount at runtime
# (provides centralized logging to jarvis-logs server)

# jarvis-config-client for service discovery
jarvis-config-client @ git+https://github.com/alexberardi/jarvis-config-client.git@main

# jarvis-settings-client for settings management
jarvis-settings-client @ git+https://github.com/alexberardi/jarvis-settings-client.git@main

# Required for GGUF conversion (vendored llama.cpp convert_hf_to_gguf.py / convert_lora_to_gguf.py)
gguf>=0.10.0
mistral_common>=1.0.0  # Unconditional import in vendored convert_hf_to_gguf.py

# Platform-specific ML libraries are installed separately:
# - llama-cpp-python 0.3.16 (built against llama.cpp commit 4227c9b)
# - mlx-lm (macOS only)
# - vllm (optional, high-performance inference)
