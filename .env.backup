# JARVIS_MODEL_NAME=.models/Mistral-Nemo-Instruct-2407-GGUF/Mistral-Nemo-Instruct-2407-Q4_K_M.gguf
# JARVIS_MODEL_NAME=.models/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf
JARVIS_INFERENCE_ENGINE=vllm
JARVIS_VLLM_TENSOR_PARALLEL_SIZE=1        # Number of GPUs
JARVIS_VLLM_GPU_MEMORY_UTILIZATION=0.9    # GPU memory usage (0.0-1.0)
JARVIS_VLLM_MAX_MODEL_LEN=4096            # Context window
JARVIS_MODEL_NAME=meta-llama/Llama-3.2-1B-Instruct
JARVIS_USE_QUANTIZATION=true
JARVIS_QUANTIZATION_TYPE=8bit  # or 4bit
# JARVIS_MODEL_NAME=gpt-4o-mini
JARVIS_MODEL_BACKEND=TRANSFORMERS
# JARVIS_MODEL_CHAT_FORMAT=chatml-function-calling
# JARVIS_REST_MODEL_URL=http://localhost:1234
# JARVIS_REST_PROVIDER=openai
# JARVIS_REST_AUTH_TYPE=bearer
# JARVIS_REST_AUTH_TOKEN=REDACTED_OPENAI_KEY
# JARVIS_REST_REQUEST_FORMAT=openai
# chatml, mistral, llama-2, llama-3
# JARVIS_MODEL_CHAT_FORMAT=gpt-oss-harmony

JARVIS_LIGHTWEIGHT_MODEL_NAME=.models/Llama-3.2-3B-Instruct-Q8_0.gguf
JARVIS_LIGHTWEIGHT_MODEL_BACKEND=VLLM
JARVIS_LIGHTWEIGHT_MODEL_CHAT_FORMAT=llama3
# chatml, mistral, llama-2, llama-3
# JARVIS_LIGHTWEIGHT_MODEL_CHAT_FORMAT=chatml


# Disable MPS fallback (optional)
DEBUG=true
JARVIS_MODEL_CONTEXT_WINDOW=8192
JARVIS_LIGHTWEIGHT_MODEL_CONTEXT_WINDOW=8192
DEBUG=true

# Debug
DEBUG_PORT=5678

# Server
SERVER_HOST=0.0.0.0
SERVER_PORT=8000

# Testing
TEST_BASE_URL=http://localhost:8000


# Llama 3.2 3B - Fast configuration
JARVIS_MODEL_CONTEXT_WINDOW=8192
JARVIS_N_THREADS=10
JARVIS_N_GPU_LAYERS=-1
JARVIS_N_BATCH=1024
JARVIS_N_UBATCH=1024
JARVIS_ENABLE_CONTEXT_CACHE=true
JARVIS_MAX_CACHE_SIZE=200
JARVIS_MAX_TOKENS=8000
# Expected: 0.3-0.8s latency, ~2-4GB memory

# # Llama 3.1 8B - Balanced configuration
# JARVIS_MODEL_CONTEXT_WINDOW=6144
# JARVIS_N_THREADS=10
# JARVIS_N_GPU_LAYERS=-1
# JARVIS_N_BATCH=512
# JARVIS_N_UBATCH=512
# JARVIS_ENABLE_CONTEXT_CACHE=true
# JARVIS_MAX_CACHE_SIZE=100
# JARVIS_MAX_TOKENS=6000
# LLAMA_METAL=true
# # Expected: 0.8-2.0s latency, ~8-12GB memory

# # Conservative - Lower memory usage
# JARVIS_MODEL_CONTEXT_WINDOW=4096
# JARVIS_N_THREADS=8
# JARVIS_N_GPU_LAYERS=-1
# JARVIS_N_BATCH=256
# JARVIS_N_UBATCH=256
# JARVIS_ENABLE_CONTEXT_CACHE=true
# JARVIS_MAX_CACHE_SIZE=50
# JARVIS_MAX_TOKENS=4000
# LLAMA_METAL=true
# Expected: Higher latency, lower memory usage

# Performance - Maximum speed
JARVIS_MODEL_CONTEXT_WINDOW=1024
JARVIS_N_THREADS=12
JARVIS_N_GPU_LAYERS=-10
JARVIS_N_BATCH=128
JARVIS_VERBOSE=true
JARVIS_N_UBATCH=1024
JARVIS_ENABLE_CONTEXT_CACHE=true
JARVIS_MAX_CACHE_SIZE=300
JARVIS_MAX_TOKENS=8000
LLAMA_METAL=false
# Expected: Fastest performance, higher memory usage

# # Debug - Verbose logging
# JARVIS_MODEL_CONTEXT_WINDOW=4096
# JARVIS_N_THREADS=10
# JARVIS_N_GPU_LAYERS=-1
# JARVIS_N_BATCH=512
# JARVIS_N_UBATCH=512
# JARVIS_ENABLE_CONTEXT_CACHE=true
# JARVIS_MAX_CACHE_SIZE=100
# JARVIS_MAX_TOKENS=7000
# JARVIS_VERBOSE=true
# LLAMA_METAL=true
# Expected: Detailed logging, normal performance