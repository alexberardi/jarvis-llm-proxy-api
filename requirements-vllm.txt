# vLLM requirements for high-performance inference
# Install with: pip install -r requirements-base.txt -r requirements-vllm.txt

# vLLM - High-performance LLM inference engine (precompiled wheel)
# Includes guided JSON / structured outputs, torch 2.9.1, triton, etc.
vllm>=0.15.0
